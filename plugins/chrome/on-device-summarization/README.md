# On-device Summarization with local http on windows on snapdragon

This sample demonstrates how to use local http to run Llama3.1 on windows on snapdragon in Chrome.

## Overview

The extension summarizes the content of the currently open tab. It uses Mozilla's [readability](https://github.com/mozilla/readability) library to extract the content of the currently active tab and displays a summary of the page generated by Llama3.1 in a side panel.

## Running this extension

### Running UI
1. Clone this repository
1. Run `npm install` in this folder to install all dependencies.
1. Run `npm run build` to build the extension.
1. Load the newly created `dist` directory in Chrome as an [unpacked extension](https://developer.chrome.com/docs/extensions/get-started/tutorial/hello-world#load-unpacked).
1. Click the extension icon to open the summary side panel.
1. Open any web page, the page's content summary will automatically be displayed in the side panel.

### Running backend local http
1. Install VSCode plugin from here https://github.com/quic/wos-ai-plugins/blob/main/plugins/vscode/qairt-code-gen/README.md#how-to-install-extension
1. Start the backend model in VSCode, which start by default on 8002 port.
1. After the model is up, the UI will fetch data from the http API 
```
Generate stream API: [Post] /api/generate_stream
Example Input Json:
{
"ask": "write a program to add numbers",
"system_prompt": "act as a code generator"
}

Example Output stream:
Sure, I'd be happy to help! Here is a simple program ...
```